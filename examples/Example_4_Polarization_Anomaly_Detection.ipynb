{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4 \n",
    "\n",
    "## Anomaly detection in fiber optic polarization\n",
    "In this example, we showcase the capabilities of Learning Entropy, implemented within our AISLEX framework, to detect anomalies in real-world data measured by the CzechLight Polarilog [1]. The original data, previously presented and analyzed in [2], were collected at a sample rate of 20 kHz with 16-bit A/D resolution across four channels.\n",
    "\n",
    "The fiber optic cable under study spans 21 km, with 10 km running alongside a railway, 5 km within a city, and 6 km in a rural area. This setup is particularly relevant as we utilize AISLEX to detect train movements based on polarization changes. The dataset includes labeled instances of train movement between two stations connected by the fiber optic cable, with the labeling provided by an automated camera-based train detection system.\n",
    "\n",
    "In this Jupyter notebook, AISLEX has been fine-tuned to detect train movement events such as acceleration and braking, which generate significant electromagnetic fields and noise. Given that part of the cable runs through an urban area, occasional false positives due to external, uncontrollable sources are expected.\n",
    "\n",
    "For this demonstration, we provide a 19-minute segment of data resampled to 150 Hz for all channels.\n",
    "\n",
    "[1] R. Vohnout, M. Šlapák, J. Jedlinský, and J. Vojtěch, “CzechLight Polarilog - Rapid Polarization Rotation Monitoring Appliance,” in 2020 22nd International Conference on Transparent Optical Networks (ICTON), Jul. 2020, pp. 1–4. doi: 10.1109/ICTON51198.2020.9203142.\n",
    "\n",
    "[2] M. Šlapák, J. Vojtěch, O. Havliš, and R. Slavík, “Monitoring of Fibre Optic Links With a Machine Learning-Assisted Low-Cost Polarimeter,” IEEE Access, vol. 8, pp. 183965–183971, 2020, doi: 10.1109/ACCESS.2020.3009524.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import spectrogram\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Get access to root folder of the notebook\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Pythonic AISLE\n",
    "from src.aisle import aisle as py_aisle\n",
    "from src.aisle import aisle_window as py_aisle_window\n",
    "\n",
    "# JAX accelerated AISLE\n",
    "from src.aisle_jax import aisle as jax_aisle\n",
    "from src.aisle_jax import aisle_window as jax_aisle_window\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the uploaded file\n",
    "file_path = r'./example_4_data.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Extract the data for each channel\n",
    "ch1 = data['ch1']\n",
    "ch2 = data['ch2']\n",
    "ch3 = data['ch3']\n",
    "ch4 = data['ch4']\n",
    "\n",
    "# Perform PCA on the combined data\n",
    "combined_data = np.vstack([ch1, ch2, ch3, ch4]).T\n",
    "pca = PCA(n_components=1)\n",
    "combined_signal = pca.fit_transform(combined_data).flatten()\n",
    "\n",
    "# Define the sampling frequency and spectogram settings\n",
    "fs = 150\n",
    "window = 'hann'\n",
    "nfft = 256\n",
    "\n",
    "# Function to plot the spectrogram\n",
    "def plot_spectrogram(data, fs, ax, title, window, nfft):\n",
    "    f, t, Sxx = spectrogram(data, fs, window=window, nfft=nfft)\n",
    "    cax = ax.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud')\n",
    "    ax.set_ylabel('Frequency [Hz]')\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_title(title)\n",
    "    return cax\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = plt.subplot2grid((2, 3), (0, 0))\n",
    "ax2 = plt.subplot2grid((2, 3), (0, 1))\n",
    "ax3 = plt.subplot2grid((2, 3), (1, 0))\n",
    "ax4 = plt.subplot2grid((2, 3), (1, 1))\n",
    "ax_combined = plt.subplot2grid((2, 3), (0, 2), rowspan=2)\n",
    "\n",
    "# Plotting\n",
    "cax1 = plot_spectrogram(ch1, fs, ax1, 'Channel 1', window, nfft)\n",
    "ax1.xaxis.set_visible(False)\n",
    "cb1 = fig.colorbar(cax1, ax=ax1)\n",
    "cb1.set_label('Intensity [dB]')\n",
    "\n",
    "cax2 = plot_spectrogram(ch2, fs, ax2, 'Channel 2', window, nfft)\n",
    "ax2.xaxis.set_visible(False)\n",
    "cb2 = fig.colorbar(cax2, ax=ax2)\n",
    "cb2.set_label('Intensity [dB]')\n",
    "\n",
    "cax3 = plot_spectrogram(ch3, fs, ax3, 'Channel 3', window, nfft)\n",
    "cb3 = fig.colorbar(cax3, ax=ax3)\n",
    "cb3.set_label('Intensity [dB]')\n",
    "\n",
    "cax4 = plot_spectrogram(ch4, fs, ax4, 'Channel 4', window, nfft)\n",
    "cb4 = fig.colorbar(cax4, ax=ax4)\n",
    "cb4.set_label('Intensity [dB]')\n",
    "\n",
    "cax_combined = plot_spectrogram(combined_signal, fs, ax_combined, 'Combined Signal', window, nfft)\n",
    "cb_combined = fig.colorbar(cax_combined, ax=ax_combined)\n",
    "cb_combined.set_label('Intensity [dB]')\n",
    "\n",
    "# Add figure title and settings\n",
    "title = (f'Spectrograms of Measured Channels and PCA Combined Signal\\n'\n",
    "         f'Settings: Sampling Frequency = {fs} Hz, Window = \"{window}\", NFFT = {nfft}')\n",
    "fig.suptitle(title, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"../test_local/images/4_Polarization__overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data scaler and scale data for training\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(combined_signal.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data setup. The parameter and ny specify how many historic values from an input vector are be used\n",
    "ny = 30\n",
    "prediction_horizon = 1\n",
    "N = data_normalized.shape[0]\n",
    "\n",
    "X = np.ones((N, 1 + ny))  # On the first position we will keep bias\n",
    "y = np.zeros((N,))\n",
    "\n",
    "# Generate X, y dynamically based on 'ny'\n",
    "for k in range(ny, N - prediction_horizon):\n",
    "    X[k, 1:] = data_normalized[k - ny : k][::1]\n",
    "    y[k] = data_normalized[k + prediction_horizon - 1]\n",
    "\n",
    "X = X[ny:-prediction_horizon, :]\n",
    "y = y[ny:-prediction_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Unit\n",
    "mu = 0.001  # Learning rate\n",
    "epochs = 5\n",
    "\n",
    "# Initialize parameters\n",
    "N = X.shape[0]\n",
    "print(f\"The total amount of samples: {N}.\")\n",
    "nw = X.shape[1]  # Get weights for neuron\n",
    "r = 1  # Set the order of polynom\n",
    "if r > 1:\n",
    "    nw = math.ceil(math.factorial(nw + r - 1) / math.factorial(r) / math.factorial(nw - 1))\n",
    "w = np.random.randn(nw) / nw\n",
    "print(f\"Total amount of trainable weights {nw}.\")\n",
    "\n",
    "# Holders for adaptation values\n",
    "e = np.zeros(N)\n",
    "yn = np.zeros(N)\n",
    "wall = np.zeros((N, nw))\n",
    "\n",
    "# Training Quadratic Neural Unit\n",
    "for epoch in range(epochs):\n",
    "    for k in range(N):\n",
    "        # Prepare polynomial input vector\n",
    "        if r > 1:\n",
    "            outer_product = np.outer(X[k], X[k])\n",
    "            upper_triangle = np.triu_indices_from(outer_product)\n",
    "            x = outer_product[upper_triangle]\n",
    "        else:\n",
    "            x = X[k, :]\n",
    "\n",
    "        yn[k] = np.dot(w, x)\n",
    "        e[k] = y[k] - yn[k]\n",
    "\n",
    "        dyndw = x  # Gradient\n",
    "\n",
    "        # Update weights\n",
    "        # Uncomment the desired weight update rule\n",
    "\n",
    "        # Pure Gradient Descent (GD)\n",
    "        # dw = mu * e[k] * dyndw\n",
    "\n",
    "        # GD with normalization\n",
    "        dw = mu / (sum(x * x)) * e[k] * dyndw  # x0=1 => division by zero is avoided\n",
    "\n",
    "        # GD with normalization and momentum\n",
    "        # dw = 0.2 * mu / (sum(x * x)) * e[k] * dyndw + 0.8 * dw\n",
    "\n",
    "        w = w + dw  # Update weights\n",
    "        wall[k, :] = w  # Store weights for analysis\n",
    "\n",
    "    epoch_error = np.mean(e**2)\n",
    "    print(f\"Epoch {epoch+1} Mean Squared Error: {epoch_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model prediction\n",
    "error = round(epoch_error, 5)\n",
    "time_domain = np.arange(0, data.shape[0]) / fs\n",
    "slice_start = 290\n",
    "slice_end = 320\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Plot observed target and neural output\n",
    "ax1 = plt.subplot(411)\n",
    "plt.title(\"Polarization changes compressed by PCA with LNU observer\")\n",
    "plt.plot(time_domain[ny:-prediction_horizon], y, \"orange\", label=r\"$y ... observed \\ targets$\")\n",
    "plt.plot(time_domain[ny:-prediction_horizon], yn, \"g\", label=r\"$y_n ... neural \\ outputs$\")\n",
    "plt.ylabel(\"[/]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlim(time_domain[0], time_domain[-1])\n",
    "plt.grid()\n",
    "\n",
    "# Plot detain of signal\n",
    "ax2 = plt.subplot(412)\n",
    "plt.title(f\"Detail of compressed polarization changes for window [{slice_start} - {slice_end}] seconds\")\n",
    "sample_range = np.arange(slice_start * fs, slice_end * fs) / fs\n",
    "plt.plot(time_domain[ny:-prediction_horizon], y, \"orange\", label=r\"$y ... observed \\ targets$\")\n",
    "plt.plot(time_domain[ny + prediction_horizon :], yn, \"g\", label=r\"$y_n ... neural \\ outputs$\")\n",
    "plt.ylabel(\"[/]\")\n",
    "plt.xlim(sample_range[0], sample_range[-1])\n",
    "plt.grid()\n",
    "\n",
    "# Plot error between observed targets and neural outputs\n",
    "ax3 = plt.subplot(413, sharex=ax1)\n",
    "plt.plot(time_domain[ny + prediction_horizon :], e, \"r\", label=r\"$e=y-y_n$\")\n",
    "plt.title(f\"Error (MSE: {error})\")\n",
    "plt.ylabel(\"[/]\")\n",
    "plt.grid()\n",
    "\n",
    "# Plot weights adaptation\n",
    "ax4 = plt.subplot(414, sharex=ax1)\n",
    "plt.plot(time_domain[ny + prediction_horizon :], wall[:, :10]), plt.ylabel(r\"$\\mathbf{w}(k)$\")\n",
    "plt.title(r\"$\\mathbf{w}(k)=\\mathbf{w}(k-1)+\\Delta \\mathbf{w}(k)$\")\n",
    "plt.grid()\n",
    "plt.xlabel(r\"$Time$  [s]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"../test_local/images/4_AIXLEX_LNU.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AISLE\n",
    "In this part we will demonstrate usage of Learning Entropy on anomaly detection. As explained in [[1](https://www.researchgate.net/publication/257200647_Learning_Entropy_Multiscale_Measure_for_Incremental_Learning)] and further refined in [[2](https://www.researchgate.net/publication/331041954_Learning_Entropy_as_a_Learning-Based_Information_Concept)], Learning Entropy detects an anomaly by analyzing the weight updates of an neuron. The basic concept expects that once model is trained, the changes to weight updates should remain relatively similar over observed window of weights unless there is some novelty in data, which should be detected by Learning Entropy, as model needs more _energy_ to achieve new state.\n",
    "\n",
    "### Execution time comparisson\n",
    "For comparison of pythonic approach and JAX accelerated execution we provide naive time measurement approach within this notebook. Please do note, that once the code has been executed, next unchanged executions are usually cached and therefore invalid for any comparisons. As a fair comparison serves the window implementation, which is too big to remain cached.\n",
    "\n",
    "#### Disclaimer\n",
    "Measuring execution times within Jupyter Notebook always brings certain amount of error and is supposed to be taken as indicative and not absolute. Single AISLE evaluation places JAX accelerated LE to an disadvantege, as on its initial run the code has to be compiled. Repeated runs are then already accelerated. If evaluated data are too short or weight count is low, the compilation time of JAX takes usually longer than native implementation.\n",
    "\n",
    "\n",
    "#### Sources\n",
    "[1] I. Bukovsky, “Learning Entropy: Multiscale Measure for Incremental Learning,” Entropy, vol. 15, no. 10, pp. 4159–4187, Sep. 2013, doi: 10.3390/e15104159.\n",
    "\n",
    "[2] I. Bukovsky, W. Kinsner, and N. Homma, “Learning Entropy as a Learning-Based Information Concept,” Entropy, vol. 21, no. 2, p. 166, Feb. 2019, doi: 10.3390/e21020166.\n",
    "\n",
    "### AISLE - Single value retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single AISLE value retrieval\n",
    "oles = (1, 2)\n",
    "alphas = (10, 15)\n",
    "\n",
    "# Python single Learning Entropy evaluation\n",
    "py_aisle_time = time.time()\n",
    "py_aisle_value = py_aisle(wall, alphas=alphas, oles=oles)\n",
    "py_aisle_time = time.time() - py_aisle_time\n",
    "\n",
    "# JAX accelerated single Learning Entropy evaluation\n",
    "jax_aisle_time = time.time()\n",
    "jax_aisle_value = jax_aisle(wall, alphas=alphas, oles=oles)\n",
    "jax_aisle_time = time.time() - jax_aisle_time\n",
    "\n",
    "msg = (\n",
    "    f\"Single Learning Entropy evaluation with pythonic approach took: {py_aisle_time:.6f} seconds.\\n\",\n",
    "    f\"Single Learning Entropy evaluation with JAX aproach took: {jax_aisle_time:.6f} seconds.\\n\\n\",\n",
    "    \"Single init evaluation with JAX is in general slower, as on its first run it has to compile parts of\",\n",
    "    \" the code.\\nRepeated usage is recommended.\\n\",\n",
    "    \"If reporting evaluation time is 0.000.., cache has been used and invalid time results are shown\",\n",
    ")\n",
    "msg = \"\".join(text for text in msg)\n",
    "print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AISLE - batch processing (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISLE setup\n",
    "oles = (3, 4)  # Order of LE\n",
    "alphas = (6, 7)  # Sensitivity of evaluation\n",
    "window = int(150 * 4.5)  # Window evaluated for each AISLE\n",
    "threshold = 0.5  # Visual threshold (red dashed line)\n",
    "\n",
    "# Sum, normalize and shift AISLE to correspond to original data\n",
    "aisle_values = np.zeros(data.shape[0])\n",
    "py_aisle_time = time.time()\n",
    "py_aisle_values = py_aisle_window(window, wall, alphas, oles)\n",
    "py_aisle_time = time.time() - py_aisle_time\n",
    "aisle_values[ny:-prediction_horizon] = np.sum(py_aisle_values, axis=1) / len(oles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all indices in descending order\n",
    "all_sorted_indices = np.argsort(aisle_values)[::-1]  # Sort all indices in descending order\n",
    "\n",
    "# Define the range within which indices are considered too close\n",
    "proximity_range = 2\n",
    "\n",
    "# Filter indices that are too close, preferring the one with the higher value\n",
    "filtered_indices = []\n",
    "for index in all_sorted_indices:\n",
    "    if aisle_values[index] < 0.1:\n",
    "        break\n",
    "    if not any(abs(index - other_index) <= proximity_range for other_index in filtered_indices):\n",
    "        filtered_indices.append(index)\n",
    "\n",
    "# Now select the top N indices after filtering\n",
    "N = 5  # Number of top indices to select\n",
    "final_indices = filtered_indices[:N]\n",
    "\n",
    "# Construct the plot title with the top three significant values\n",
    "top_values_str = \"times:\" + \", \".join([f\" {idx/fs:.2f}\" for idx in final_indices])\n",
    "plot_title = (\n",
    "    f\"py_AISLE weight evaluation of ECG signal in {py_aisle_time:.2f} sec.\\n\",\n",
    "    f\"OLEs: {oles}, Alphas: {alphas}, window: {window}.\\n\",\n",
    "    f\"Top 5 significant detections at {top_values_str} sec.\\n\",\n",
    "    \"Area with train movement is marked with light green.\",\n",
    ")\n",
    "plot_title = \"\".join(text for text in plot_title)\n",
    "\n",
    "start = 21450\n",
    "train_dt = start + 600*150\n",
    "roi_ranges = (time_domain[start], time_domain[train_dt])\n",
    "# roi_ranges = (start, train_dt)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "fig.suptitle(plot_title)\n",
    "plt.plot(time_domain, aisle_values, label=\"AISLE\")\n",
    "plt.hlines([0.5], 0, aisle_values.shape[0] // fs, colors=\"red\", linestyles=\"dashed\")\n",
    "plt.ylabel(r\"$[/]$\")\n",
    "plt.xlabel(r\"$Time$  [s]\")\n",
    "\n",
    "# Fill specified ROI ranges with green background\n",
    "plt.fill_between(\n",
    "    x=np.arange(roi_ranges[0] + ny / fs, roi_ranges[1] + 1), y1=-0.05, y2=1.05, color=\"green\", alpha=0.2\n",
    ")\n",
    "\n",
    "# Places crosses on top indices\n",
    "for index in final_indices:\n",
    "    if index < len(aisle_values):  # Check to avoid indexing errors\n",
    "        plt.scatter(index / fs, aisle_values[index], color=\"red\", marker=\"x\", s=50)\n",
    "\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"../test_local/images/4_pyAISLE.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AISLE batch processing (JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISLEX setup\n",
    "oles = (3, 4)  # Order of LE\n",
    "alphas = (6, 7)  # Sensitivity of evaluation\n",
    "window = int(150 * 4.5)  # Window evaluated for each AISLE\n",
    "threshold = 0.5  # Visual threshold (red dashed line)\n",
    "window_chunks = 35000\n",
    "\n",
    "# Sum, normalize and shift AISLE to correspond to original data\n",
    "aisle_values = np.zeros(data.shape[0])\n",
    "jax_aisle_time = time.time()\n",
    "jax_aisle_values = jax_aisle_window(window, wall, alphas, oles, window_chunks)\n",
    "jax_aisle_time = time.time() - jax_aisle_time\n",
    "aisle_values[ny + prediction_horizon :] = np.sum(jax_aisle_values, axis=1) / len(oles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all indices in descending order\n",
    "all_sorted_indices = np.argsort(aisle_values)[::-1]  # Sort all indices in descending order\n",
    "\n",
    "# Define the range within which indices are considered too close\n",
    "proximity_range = 2\n",
    "\n",
    "# Filter indices that are too close, preferring the one with the higher value\n",
    "filtered_indices = []\n",
    "for index in all_sorted_indices:\n",
    "    if aisle_values[index] < 0.1:\n",
    "        break\n",
    "    if not any(abs(index - other_index) <= proximity_range for other_index in filtered_indices):\n",
    "        filtered_indices.append(index)\n",
    "\n",
    "# Now select the top N indices after filtering\n",
    "N = 5  # Number of top indices to select\n",
    "final_indices = filtered_indices[:N]\n",
    "\n",
    "# Construct the plot title with the top three significant values\n",
    "top_values_str = \"times:\" + \", \".join([f\" {idx/fs:.2f}\" for idx in final_indices])\n",
    "plot_title = (\n",
    "    f\"jax_AISLE weight evaluation of ECG signal in {jax_aisle_time:.2f} sec.\\n\",\n",
    "    f\"OLEs: {oles}, Alphas: {alphas}, window: {window}.\\n\",\n",
    "    f\"Top 5 significant detections at {top_values_str} sec.\\n\",\n",
    "    \"Area with train movement is marked with light green.\",\n",
    ")\n",
    "plot_title = \"\".join(text for text in plot_title)\n",
    "\n",
    "start = 21450\n",
    "train_dt = start + 600*150\n",
    "roi_ranges = (time_domain[start], time_domain[train_dt])\n",
    "# roi_ranges = (start, train_dt)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "fig.suptitle(plot_title)\n",
    "plt.plot(time_domain, aisle_values, label=\"AISLEX\")\n",
    "plt.hlines([0.5], 0, aisle_values.shape[0] // fs, colors=\"red\", linestyles=\"dashed\")\n",
    "plt.ylabel(r\"$[/]$\")\n",
    "plt.xlabel(r\"$Time$  [s]\")\n",
    "\n",
    "# Fill specified ROI ranges with green background\n",
    "plt.fill_between(\n",
    "    x=np.arange(roi_ranges[0] + ny / fs, roi_ranges[1] + 1), y1=-0.05, y2=1.05, color=\"green\", alpha=0.2\n",
    ")\n",
    "\n",
    "# Places crosses on top indices\n",
    "for index in final_indices:\n",
    "    if index < len(aisle_values):  # Check to avoid indexing errors\n",
    "        plt.scatter(index / fs, aisle_values[index], color=\"red\", marker=\"x\", s=50)\n",
    "\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"../test_local/images/4_jaxAISLE.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISLE with pythonic approach\n",
    "%timeit -n100 -r25 py_aisle(wall, alphas=alphas, oles=oles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISLE with JAX accelerated approach\n",
    "%timeit -n100 -r25 jax_aisle(wall, alphas=alphas, oles=oles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISLE window evaluation with pythonic approach\n",
    "%timeit -n5 -r10 py_aisle_window(window, wall, alphas, oles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISLE window evaluation with JAX accelerated approach\n",
    "%timeit -n5 -r10 jax_aisle_window(window, wall, alphas, oles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
